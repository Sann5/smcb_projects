---
title: "Project 10"
author: "Santiago Castro Dau, June Monge, Rachita Kumar, Sarah Lötscher"
date: '2022-05-05'
output: 
  pdf_document:
    latex_engine: xelatex
    extra_dependencies:
      amsmath
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Problem 27: Uniqueness of predictions from the lasso 
###  1. 
```{r}

```

## Problem 28: Bayesian priors as regularizers 

```{r}

```

## Problem 29: Variable selection under various norms
:

```{r, echo = FALSE}
library("ggplot2")
library("caret")
library("glmnet")
library("pROC")
library("magrittr")
library("dplyr")

```


### 1. Load the data and construct the design matrix X and response variable y, respectively. Randomly split the data into training set (70%) and test set (30%). For reproducibility set the seed to 42 in the beginning.

```{r}
#Read in dataset
set.seed(42)
load("./yeastStorey.rda")
```

```{r}
#Sample data
Sampledata<- data$Marker %>%
  createDataPartition(p = 0.7,list = FALSE)

#Create training data
Traindata<-data[Sampledata, ]

#Create test data
Testdata<-data[-Sampledata, ]

```


```{r}
#Create X and y train
X_train <- apply(as.matrix.noquote(Traindata[,-1]), 2,as.numeric)

Y_train<- Traindata[,1]

#Create X and y test
X_test <- apply(as.matrix.noquote(Testdata[,-1]), 2, as.numeric)

Y_test <- Testdata[,1]

```



### 2. Using 10-fold cross-validation, find the optimum λ and optimum α using elastic-net model on the training set. Fit the final model with the optimal parameters on the training set. For reducing computation time restrict the search space of α to {0, 0.1, 0.2, · · · , 1}.

```{r}
#Get alpha vector  α = {0, 0.1, 0.2, · · · , 1}
alphavec= seq(0, 1, 0.1)

```

```{r}

#Get the foldid
foldid <- sample(1:10, size=length(Sampledata), replace = TRUE )


#Find best lambda using 10-fold cross-validation
crossval <- lapply(alphavec, function(n)
  {cv.glmnet(X_train, Y_train, alpha=n, family = "binomial",foldid = foldid, type.measure="mse")})


#Make dataset with the new values for plotting
dd<- do.call(rbind,lapply(1:length(alphavec), function(x){
  cbind.data.frame(alphavec[x],crossval[[x]]$lambda,crossval[[x]]$cvm)}))

colnames(dd)<- c('alpha','lambda','cvm')

```


### 3. Predict the response on the test dataset using the final model. Plot the cross-validation error as a function of log λ, trace curve of coefficients as a function of log λ, and the ROC curve for the optimal α. Lastly, report the corresponding AUC (area under the curve) of the ROC curve and the variables selected.


```{r}
#Predict the response on the test dataset

#Get alpha and lambda
param <- filter(dd ,cvm== min(cvm))

#Fit model using trainingdata
model <-glmnet(X_train, Y_train, alpha = param$alpha, family = "binomial" ,lambda = param$lambda)


#Predict Y of testdata
predY <- predict(model, newx = X_test, type = "class")



```


```{r}
#Plot cross-validation error as a function of log λ

Acc <- roc(Y_test,as.vector(predY))
plot(Acc, legacy.axes=TRUE)
```


```{r}
#Plot trace curve of coefficients as a function of log λ
plot(crossval[[11]]$glmnet.fit,"lambda",label=TRUE)
abline(v=log(crossval[[11]]$lambda.min),col="red")
```


```{r}
#ROC curve for the optimal α
plot(crossval[[11]])
```


```{r}
#Get the AUC
AUC$auc
```

```{r}
#Variables/predictors selected
predictors<-coef(crossval[[11]], param$lambda)
predictors@Dimnames[[1]][predictors@i]

```
