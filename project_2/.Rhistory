# Update observed log-likelihood
## TODO:
if (prod(a) + prod(b) == 0) {
ll_obs = 0
} else {
ll_obs = log(prod(a) + prod(b))
}
# Recompute difference between two iterations
diff = ll_obs - ll_obs_old
### YOUR CODE ENDS ###
############# M-step #############
### YOUR CODE STARTS ###
# Recompute priors (mixture weights)
lambda = rowSums(gamma) / N
# Recompute probability of heads for each coin
num_h=rowSums(D)
count_h=gamma%*%num_h
count_t=(1-gamma)%*%(dim(D)[2]-num_h)
theta=t(count_h/(count_h+count_t))
### YOUR CODE ENDS ###
}
return(list(ll_hid = ll_hid, ll_obs = ll_obs, lambda = lambda, theta = theta, gamma = gamma))
}
res <- coin_EM(D, k, lambda, theta)
## YOUR CODE ##
res$theta
## YOUR CODE ##
res$lambda
P
##' This function implements the EM algorithm for the coin toss problem
##' @param D Data matrix of dimensions 100-by-N, where N is the number of observations
##' @param k Number of coins
##' @param lambda Vector of mixture weights
##' @param theta Vector of probabilities of obtaining heads
##' @param tolerance A threshold used to check convergence
coin_EM <- function(D, k, lambda, theta, tolerance = 1e-2) {
# expected complete-data (hidden) log-likelihood
ll_hid <- -Inf
# observed log-likelihood
ll_obs <- -Inf
# difference between two iterations
diff <- Inf
# number of observations
N <- nrow(D)
# responsibilities
gamma <- matrix(0, nrow = k, ncol = N)
# run the E-step and M-step until convergence
while (diff > tolerance) {
# store old likelihood
ll_obs_old <- ll_obs
############# E-step #############
### YOUR CODE STARTS ###
# Compute the responsibilities
P <- rbind(theta[1]^rowSums(D) * (1 - theta[1])^(dim(D)[2] - rowSums(D)),
theta[2]^rowSums(D) * (1 - theta[2])^(dim(D)[2] - rowSums(D)))
a <- lambda[1] * P[1, ]
b <- lambda[2] * P[2, ]
gamma[1, ] <- a/(a + b)
gamma[2, ] <- b/(a + b)
# Update expected complete-data (hidden) log-likelihood
ll_hid = sum(gamma * log(lambda*P))
# Update observed log-likelihood
## TODO:
if (prod(a) + prod(b) == 0) {
ll_obs = 0
} else {
ll_obs = log(prod(a) + prod(b))
}
# Recompute difference between two iterations
diff = ll_obs - ll_obs_old
### YOUR CODE ENDS ###
############# M-step #############
### YOUR CODE STARTS ###
# Recompute priors (mixture weights)
lambda = rowSums(gamma) / N
# Recompute probability of heads for each coin
num_h=rowSums(D)
count_h=gamma%*%num_h
count_t=(1-gamma)%*%(dim(D)[2]-num_h)
theta=t(count_h/(count_h+count_t))
### YOUR CODE ENDS ###
}
return(list(ll_hid = ll_hid, ll_obs = ll_obs, lambda = lambda, theta = theta, gamma = gamma))
}
res <- coin_EM(D, k, lambda, theta)
## YOUR CODE ##
res$theta
## YOUR CODE ##
res$lambda
## YOUR CODE ##
res$gamma[1, ] > res$gamma[2, ]
## YOUR CODE ##
sum(res$gamma[1, ] > res$gamma[2, ])
## YOUR CODE ##
# Coin 1
obs_1 = sum(gamma[1,]>gamma[2,])
obs_1
# Coin 2
N-obs_1
## YOUR CODE ##
# Coin 1
obs_1 = sum(res$gamma[1,]>res$gamma[2,])
obs_1
# Coin 2
N-obs_1
## YOUR CODE ##
heatmap(t(gamma),Rowv=NA,Colv=NA)
## YOUR CODE ##
heatmap(t(res$gamma),Rowv=NA,Colv=NA)
## YOUR CODE ##
heatmap(res$gamma,Rowv=NA,Colv=NA)
t(
res$gamma
res$gamma
res$gamma
res$theta
rowSums(D)
# Number of coins
k <- 2
# Mixture weights (a vector of length k)
lambda <- c(0.5, 0.5)
# Probabilities of obtaining heads (a vector of length k)
theta <- runif(k)
##' This function implements the EM algorithm for the coin toss problem
##' @param D Data matrix of dimensions 100-by-N, where N is the number of observations
##' @param k Number of coins
##' @param lambda Vector of mixture weights
##' @param theta Vector of probabilities of obtaining heads
##' @param tolerance A threshold used to check convergence
coin_EM <- function(D, k, lambda, theta, tolerance = 1e-2) {
# expected complete-data (hidden) log-likelihood
ll_hid <- -Inf
# observed log-likelihood
ll_obs <- -Inf
# difference between two iterations
diff <- Inf
# number of observations
N <- nrow(D)
# responsibilities
gamma <- matrix(0, nrow = k, ncol = N)
# run the E-step and M-step until convergence
while (diff > tolerance) {
# store old likelihood
ll_obs_old <- ll_obs
############# E-step #############
### YOUR CODE STARTS ###
# Compute the responsibilities
P <- rbind(theta[1]^rowSums(D) * (1 - theta[1])^(dim(D)[2] - rowSums(D)),
theta[2]^rowSums(D) * (1 - theta[2])^(dim(D)[2] - rowSums(D)))
a <- lambda[1] * P[1, ]
b <- lambda[2] * P[2, ]
gamma[1, ] <- a/(a + b)
gamma[2, ] <- b/(a + b)
# Update expected complete-data (hidden) log-likelihood
ll_hid = sum(gamma * log(lambda*P))
# Update observed log-likelihood
## TODO:
if (prod(a) + prod(b) == 0) {
ll_obs = 0
} else {
ll_obs = log(prod(a) + prod(b))
}
# Recompute difference between two iterations
diff = ll_obs - ll_obs_old
### YOUR CODE ENDS ###
############# M-step #############
### YOUR CODE STARTS ###
# Recompute priors (mixture weights)
lambda = rowSums(gamma) / N
# Recompute probability of heads for each coin
num_h=rowSums(D)
count_h=gamma%*%num_h
count_t=(1-gamma)%*%(dim(D)[2]-num_h)
theta=t(count_h/(count_h+count_t))
### YOUR CODE ENDS ###
}
return(list(ll_hid = ll_hid, ll_obs = ll_obs, lambda = lambda, theta = theta, gamma = gamma))
}
## YOUR CODE ##
res$theta
## YOUR CODE ##
res$lambda
## YOUR CODE ##
heatmap(t(res$gamma),Rowv=NA,Colv=NA)
theta
dim(D)[2]
dim(D)[1]
a = 0.6 * 0.7^4 * 0.3^6
b = 0.4 * 0.4^4 * 0.6^6
a/(a+b)
b/(a+b)
a = 0.6 * 0.7^8 * 0.3^2
b = 0.4 * 0.4^8 * 0.6^2
a/(a+b)
b/(a+b)
(0.97+0.18)/2
(0.82+0.103)/2
set.seed(2022)
library(tidyverse)
library(vroom)
library(vroom)
# read the data into D
D <- vroom("./coinflip.csv", show_col_types = FALSE)
# check the dimension of D
all(dim(D) == c(200, 100))
# Number of coins
k <- 2
# Mixture weights (a vector of length k)
lambda <- c(0.5, 0.5)
# Probabilities of obtaining heads (a vector of length k)
theta <- runif(k)
##' This function implements the EM algorithm for the coin toss problem
##' @param D Data matrix of dimensions 100-by-N, where N is the number of observations
##' @param k Number of coins
##' @param lambda Vector of mixture weights
##' @param theta Vector of probabilities of obtaining heads
##' @param tolerance A threshold used to check convergence
coin_EM <- function(D, k, lambda, theta, tolerance = 1e-2) {
# expected complete-data (hidden) log-likelihood
ll_hid <- -Inf
# observed log-likelihood
ll_obs <- -Inf
# difference between two iterations
diff <- Inf
# number of observations
N <- nrow(D)
# responsibilities
gamma <- matrix(0, nrow = k, ncol = N)
# run the E-step and M-step until convergence
while (diff > tolerance) {
# store old likelihood
ll_obs_old <- ll_obs
############# E-step #############
### YOUR CODE STARTS ###
# Compute the responsibilities
P <- rbind(theta[1]^rowSums(D) * (1 - theta[1])^(dim(D)[2] - rowSums(D)),
theta[2]^rowSums(D) * (1 - theta[2])^(dim(D)[2] - rowSums(D)))
a <- lambda[1] * P[1, ]
b <- lambda[2] * P[2, ]
gamma[1, ] <- a/(a + b)
gamma[2, ] <- b/(a + b)
# Update expected complete-data (hidden) log-likelihood
ll_hid = sum(gamma * log(lambda*P))
# Update observed log-likelihood
if (prod(a) + prod(b) == 0) {
ll_obs = 0
} else {
ll_obs = log(prod(a) + prod(b))
}
# Recompute difference between two iterations
diff = ll_obs - ll_obs_old
### YOUR CODE ENDS ###
############# M-step #############
### YOUR CODE STARTS ###
# Recompute priors (mixture weights)
lambda = rowSums(gamma) / N
# Recompute probability of heads for each coin
num_h=rowSums(D)
count_h=gamma%*%num_h
count_t=(1-gamma)%*%(dim(D)[2]-num_h)
theta=t(count_h/(count_h+count_t))
### YOUR CODE ENDS ###
}
return(list(ll_hid = ll_hid, ll_obs = ll_obs, lambda = lambda, theta = theta, gamma = gamma))
}
res <- coin_EM(D, k, lambda, theta)
## YOUR CODE ##
res$theta
## YOUR CODE ##
res$lambda
## YOUR CODE ##
heatmap(t(res$gamma),Rowv=NA,Colv=NA)
## YOUR CODE ##
# Coin 1
obs_1 = sum(res$gamma[1,]>res$gamma[2,])
obs_1
# Coin 2
dim(D)[1]-obs_1
res$ll_hid
res$ll_obs
##' This function implements the EM algorithm for the coin toss problem
##' @param D Data matrix of dimensions 100-by-N, where N is the number of observations
##' @param k Number of coins
##' @param lambda Vector of mixture weights
##' @param theta Vector of probabilities of obtaining heads
##' @param tolerance A threshold used to check convergence
coin_EM <- function(D, k, lambda, theta, tolerance = 1e-2) {
# expected complete-data (hidden) log-likelihood
ll_hid <- -Inf
# observed log-likelihood
ll_obs <- -Inf
# difference between two iterations
diff <- Inf
# number of observations
N <- nrow(D)
# responsibilities
gamma <- matrix(0, nrow = k, ncol = N)
# run the E-step and M-step until convergence
while (diff > tolerance) {
# store old likelihood
ll_obs_old <- ll_obs
############# E-step #############
### YOUR CODE STARTS ###
# Compute the responsibilities
P <- rbind(theta[1]^rowSums(D) * (1 - theta[1])^(dim(D)[2] - rowSums(D)),
theta[2]^rowSums(D) * (1 - theta[2])^(dim(D)[2] - rowSums(D)))
a <- lambda[1] * P[1, ]
b <- lambda[2] * P[2, ]
gamma[1, ] <- a/(a + b)
gamma[2, ] <- b/(a + b)
# Update expected complete-data (hidden) log-likelihood
ll_hid = sum(gamma * log(lambda*P))
# Update observed log-likelihood
if (prod(a) + prod(b) == 0) {
ll_obs = 0
} else {
ll_obs = log(prod(a) + prod(b))
}
# Recompute difference between two iterations
diff = ll_obs - ll_obs_old
print(diff)
### YOUR CODE ENDS ###
############# M-step #############
### YOUR CODE STARTS ###
# Recompute priors (mixture weights)
lambda = rowSums(gamma) / N
# Recompute probability of heads for each coin
num_h=rowSums(D)
count_h=gamma%*%num_h
count_t=(1-gamma)%*%(dim(D)[2]-num_h)
theta=t(count_h/(count_h+count_t))
### YOUR CODE ENDS ###
}
return(list(ll_hid = ll_hid, ll_obs = ll_obs, lambda = lambda, theta = theta, gamma = gamma))
}
res <- coin_EM(D, k, lambda, theta)
# number of observations
N <- nrow(D)
# responsibilities
gamma <- matrix(0, nrow = k, ncol = N)
# store old likelihood
ll_obs_old <- ll_obs
# store old likelihood
ll_obs_old <- ll_obs
# observed log-likelihood
ll_obs <- -Inf
# difference between two iterations
diff <- Inf
# expected complete-data (hidden) log-likelihood
ll_hid <- -Inf
# Compute the responsibilities
P <- rbind(theta[1]^rowSums(D) * (1 - theta[1])^(dim(D)[2] - rowSums(D)),
theta[2]^rowSums(D) * (1 - theta[2])^(dim(D)[2] - rowSums(D)))
a <- lambda[1] * P[1, ]
b <- lambda[2] * P[2, ]
gamma[1, ] <- a/(a + b)
gamma[2, ] <- b/(a + b)
View(P)
View(P)
rowSums(D)
# Compute the responsibilities
P <- rbind((theta[1]^rowSums(D)) * ((1 - theta[1])^(dim(D)[2] - rowSums(D))),
(theta[2]^rowSums(D)) * ((1 - theta[2])^(dim(D)[2] - rowSums(D)))
a <- lambda[1] * P[1, ]
# Compute the responsibilities
P <- rbind((theta[1]^rowSums(D)) * ((1 - theta[1])^(dim(D)[2] - rowSums(D))),
(theta[2]^rowSums(D)) * ((1 - theta[2])^(dim(D)[2] - rowSums(D))))
a
theta
theta[1]^rowSums(D)
(1 - theta[1])^(dim(D)[2] - rowSums(D))
dim(D)[2]
dim(D)[2] - rowSums(D))
dim(D)[2] - rowSums(D)
theta[1]^rowSums(D)
theta[1]
rowSums(D)
theta[1]^43
sum(log(apply(P*lambda, 2, sum)))
?apply(array, margin, ...)
P*lambda
apply(P*lambda, 2, sum)
##' This function implements the EM algorithm for the coin toss problem
##' @param D Data matrix of dimensions 100-by-N, where N is the number of observations
##' @param k Number of coins
##' @param lambda Vector of mixture weights
##' @param theta Vector of probabilities of obtaining heads
##' @param tolerance A threshold used to check convergence
coin_EM <- function(D, k, lambda, theta, tolerance = 1e-2) {
# expected complete-data (hidden) log-likelihood
ll_hid <- -Inf
# observed log-likelihood
ll_obs <- -Inf
# difference between two iterations
diff <- Inf
# number of observations
N <- nrow(D)
# responsibilities
gamma <- matrix(0, nrow = k, ncol = N)
# run the E-step and M-step until convergence
while (diff > tolerance) {
# store old likelihood
ll_obs_old <- ll_obs
############# E-step #############
### YOUR CODE STARTS ###
# Compute the responsibilities
P <- rbind(theta[1]^rowSums(D) * (1 - theta[1])^(dim(D)[2] - rowSums(D)),
theta[2]^rowSums(D) * (1 - theta[2])^(dim(D)[2] - rowSums(D)))
a <- lambda[1] * P[1, ]
b <- lambda[2] * P[2, ]
gamma[1, ] <- a/(a + b)
gamma[2, ] <- b/(a + b)
# Update expected complete-data (hidden) log-likelihood
ll_hid = sum(gamma * log(lambda*P))
# Update observed log-likelihood
ll_obs = sum(log(apply(P*lambda, 2, sum)))
# Recompute difference between two iterations
diff = abs(ll_obs - ll_obs_old)
### YOUR CODE ENDS ###
############# M-step #############
### YOUR CODE STARTS ###
# Recompute priors (mixture weights)
lambda = rowSums(gamma) / N
# Recompute probability of heads for each coin
num_h=rowSums(D)
count_h=gamma%*%num_h
count_t=(1-gamma)%*%(dim(D)[2]-num_h)
theta=t(count_h/(count_h+count_t))
### YOUR CODE ENDS ###
}
return(list(ll_hid = ll_hid, ll_obs = ll_obs, lambda = lambda, theta = theta, gamma = gamma))
}
res <- coin_EM(D, k, lambda, theta)
## YOUR CODE ##
res$theta
## YOUR CODE ##
heatmap(t(res$gamma),Rowv=NA,Colv=NA)
##' This function implements the EM algorithm for the coin toss problem
##' @param D Data matrix of dimensions 100-by-N, where N is the number of observations
##' @param k Number of coins
##' @param lambda Vector of mixture weights
##' @param theta Vector of probabilities of obtaining heads
##' @param tolerance A threshold used to check convergence
coin_EM <- function(D, k, lambda, theta, tolerance = 1e-2) {
# expected complete-data (hidden) log-likelihood
ll_hid <- -Inf
# observed log-likelihood
ll_obs <- -Inf
# difference between two iterations
diff <- Inf
# number of observations
N <- nrow(D)
# responsibilities
gamma <- matrix(0, nrow = k, ncol = N)
# run the E-step and M-step until convergence
while (diff > tolerance) {
# store old likelihood
ll_obs_old <- ll_obs
############# E-step #############
### YOUR CODE STARTS ###
# Compute the responsibilities
P <- rbind(theta[1]^rowSums(D) * (1 - theta[1])^(dim(D)[2] - rowSums(D)),
theta[2]^rowSums(D) * (1 - theta[2])^(dim(D)[2] - rowSums(D)))
a <- lambda[1] * P[1, ]
b <- lambda[2] * P[2, ]
gamma[1, ] <- a/(a + b)
gamma[2, ] <- b/(a + b)
# Update expected complete-data (hidden) log-likelihood
ll_hid = sum(gamma * log(lambda*P))
# Update observed log-likelihood
ll_obs = sum(log(apply(P*lambda, 2, sum)))
# Recompute difference between two iterations
diff = abs(ll_obs - ll_obs_old)
### YOUR CODE ENDS ###
############# M-step #############
### YOUR CODE STARTS ###
# Recompute priors (mixture weights)
lambda = rowSums(gamma) / N
# Recompute probability of heads for each coin
num_h=rowSums(D)
count_h=gamma%*%num_h
count_t=(1-gamma)%*%(dim(D)[2]-num_h)
theta=t(count_h/(count_h+count_t))
### YOUR CODE ENDS ###
}
return(list(ll_hid = ll_hid, ll_obs = ll_obs, lambda = lambda, theta = theta, gamma = gamma))
}
res <- coin_EM(D, k, lambda, theta)
## YOUR CODE ##
res$theta
## YOUR CODE ##
res$lambda
## YOUR CODE ##
heatmap(t(res$gamma),Rowv=NA,Colv=NA)
## YOUR CODE ##
# Coin 1
obs_1 = sum(res$gamma[1,]>res$gamma[2,])
obs_1
# Coin 2
dim(D)[1]-obs_1
sum(res$lambda)
res$ll_hid
res$ll_obs
gamma
res$gamma
## YOUR CODE ##
heatmap(t(res$gamma),Rowv=NA,Colv=NA)
?heatmap
## YOUR CODE ##
heatmap(t(res$gamma),Rowv=NA,Colv=NA, scale="row")
## YOUR CODE ##
heatmap(t(res$gamma),Rowv=NA,Colv=NA, scale="column")
