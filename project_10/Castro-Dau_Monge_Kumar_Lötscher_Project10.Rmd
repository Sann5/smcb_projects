---
title: "Project 10"
author: "Santiago Castro Dau, June Monge, Rachita Kumar, Sarah Lötscher"
date: '2022-05-05'
output: 
  pdf_document:
    latex_engine: xelatex
    extra_dependencies:
      amsmath
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Problem 27: Uniqueness of predictions from the lasso 
###  1. 
{r}


We will prove the statement of the exercise by contradiction. Let us define two solutions $\hat{\beta_1}$ and $\hat{\beta_2}$ $\in S$ such that $X\hat{\beta_1}\ne X\hat{\beta_2}$. Since the solution set $S$ of the convex minimization problem is convex, $\alpha \hat{\beta_1}+(1-\alpha)\hat{\beta_2} \in S$, and given the strict convexity of the loss function, $f(\alpha \hat{\beta_1}+(1-\alpha)\hat{\beta_2})< \alpha f(\hat{\beta_1}) + (1-\alpha)f(\hat{\beta_2})$, we obtain the following:
$$
\frac{1}{2}||y-X(\alpha \hat{\beta_1}+(1-\alpha)\hat{\beta_2})||_2^2 + \lambda || \alpha \hat{\beta_1}+(1-\alpha)\hat{\beta_2} ||_1 < \alpha\left( \frac{1}{2}||y-X\hat{\beta_1})||_2^2 + \lambda ||\hat{\beta_1}||_1\right) + (1-\alpha)\left( \frac{1}{2}||y-X\hat{\beta_2})||_2^2 + \lambda ||\hat{\beta_2}||_1\right)\\
\frac{1}{2}||y-X(\alpha \hat{\beta_1}+(1-\alpha)\hat{\beta_2})||_2^2 + \lambda || \alpha \hat{\beta_1}+(1-\alpha)\hat{\beta_2} ||_1 < \alpha c^* +(1-\alpha)c^* \\

\frac{1}{2}||y-X(\alpha \hat{\beta_1}+(1-\alpha)\hat{\beta_2})||_2^2 + \lambda || \alpha \hat{\beta_1}+(1-\alpha)\hat{\beta_2} ||_1 < c^* \\
$$
Note that due to the convexity of the problem  $\{\alpha \hat{\beta_1}+(1-\alpha)\hat{\beta_2},\;\hat{\beta_1},\;\hat{\beta_2}\} \in S, \forall \alpha \in (0,1)$, thus, the $c^$ correspoding to the solution $\alpha \hat{\beta_1}+(1-\alpha)\hat{\beta_2}$ should not be lower than $c^$, as we have stablished that $\hat{\beta_1} \text{ and } \hat{\beta_2} \in S$ give rise to the optimal criterion value $c^*$.  Overall, this implies that $X\hat{\beta_1} = X\hat{\beta_2}$.
###  2.
As proven above, given that $X\hat{\beta_1} = X\hat{\beta_2}$, then it holds that:
$$
\frac{1}{2}||y-X\hat{\beta_1})||_2^2 = \frac{1}{2}||y-X\hat{\beta_2})||_2^2
$$
Given that the optimal criterion value $c^*$ is identical for both solutions, we obtain that
$$
\frac{1}{2}||y-X\hat{\beta_1})||_2^2 + \lambda ||\hat{\beta_1}||_1 = \frac{1}{2}||y-X\hat{\beta_1})||_2^2 + \lambda ||\hat{\beta_2}||_1\\
\lambda ||\hat{\beta_1}||_1 = \lambda ||\hat{\beta_2}||_1\\
$$
This shows that $||\hat{\beta_1}||_1 = ||\hat{\beta_2}||_1$.


## Problem 28: Bayesian priors as regularizers 

```{r}

```

## Problem 29: Variable selection under various norms
:

```{r, echo = FALSE}
library("ggplot2")
library("caret")
library("glmnet")
library("pROC")
library("dplyr")

```


### 1. Load the data and construct the design matrix X and response variable y, respectively. Randomly split the data into training set (70%) and test set (30%). For reproducibility set the seed to 42 in the beginning.

```{r}
#Read in dataset
set.seed(42)
load("./yeastStorey.rda")
```

```{r}
#Sample data
Sampledata<- createDataPartition(data$Marker,p = 0.7,list = FALSE)

#Create training data
Traindata<-data[Sampledata, ]

#Create test data
Testdata<-data[-Sampledata, ]

```


```{r}
#Create X and y train
X_train <- apply(as.matrix.noquote(Traindata[,-1]), 2,as.numeric)

Y_train<- Traindata[,1]

#Create X and y test
X_test <- apply(as.matrix.noquote(Testdata[,-1]), 2, as.numeric)

Y_test <- Testdata[,1]

```



### 2. Using 10-fold cross-validation, find the optimum λ and optimum α using elastic-net model on the training set. Fit the final model with the optimal parameters on the training set. For reducing computation time restrict the search space of α to {0, 0.1, 0.2, · · · , 1}.

```{r}
#Get alpha vector  α = {0, 0.1, 0.2, · · · , 1}
alphavec= seq(0, 1, 0.1)

```

```{r}

#Get the foldid
foldid <- sample(1:10, size=length(Sampledata), replace = TRUE )


#Find best lambda using 10-fold cross-validation
crossval <- lapply(alphavec, function(n)
  {cv.glmnet(X_train, Y_train, alpha=n, family = "binomial",foldid = foldid, type.measure="mse")})


#Make dataset with the new values for plotting
dd<- do.call(rbind,lapply(1:length(alphavec), function(x){
  cbind.data.frame(alphavec[x],crossval[[x]]$lambda,crossval[[x]]$cvm)}))

colnames(dd)<- c('alpha','lambda','cvm')

```


### 3. Predict the response on the test dataset using the final model. Plot the cross-validation error as a function of log λ, trace curve of coefficients as a function of log λ, and the ROC curve for the optimal α. Lastly, report the corresponding AUC (area under the curve) of the ROC curve and the variables selected.


```{r}
#Predict the response on the test dataset

#Get alpha and lambda
param <- filter(dd ,cvm== min(cvm))

#Fit model using trainingdata
model <-glmnet(X_train, Y_train, alpha = param$alpha, family = "binomial" ,lambda = param$lambda)


#Predict Y of testdata
predY <- predict(model, newx = X_test, type = "class")



```


```{r}
#Plot cross-validation error as a function of log λ

Acc <- roc(Y_test,as.vector(predY))
plot(Acc, legacy.axes=TRUE)
```


```{r}
#Plot trace curve of coefficients as a function of log λ
plot(crossval[[11]]$glmnet.fit,"lambda",label=TRUE)
```


```{r}
#ROC curve for the optimal α
plot(crossval[[11]])
```


```{r}
#Get the AUC
AUC$auc
```

```{r}
#Variables/predictors selected
predictors<-coef(crossval[[11]], param$lambda)
predictors@Dimnames[[1]][predictors@i]

```
